{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "compound-exclusion",
   "metadata": {},
   "source": [
    "#### Lecture 1. NLP basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-alloy",
   "metadata": {},
   "source": [
    "- NLP 는 가장 빠르게 AI의 변화를 선도하는 분야\n",
    "- Low-level parsing\n",
    "    - tokenization : 문장을 특정 토큰들의 sequence로 파악\n",
    "    - stemming : 어미의 변화 속에서도 같은 의미를 가진 단어임을 찾아냄(어근 추출)\n",
    "- word and phrase level parsing\n",
    "    - Named entity recognition(NER) : 고유명사 인식\n",
    "    - part of speech(POS) tagging : 문장에서 단어의 품사 파악\n",
    "- sentence level\n",
    "    - sentiment anaylsis : 문장에서 들어나는 감정 파악(긍정/부정)\n",
    "    - machine translation : 문장 단위로 이해하고 번역\n",
    "- multi-sentence and paragraph level\n",
    "    - entailment prediction : 두 문장간의 내포, 모순 관계 예측\n",
    "    - question answering : 문장을 정확히 독해하고 정답을 제시\n",
    "    - dialog system : chat-bot\n",
    "- Text mining : 문서 군집화, 사회 현상 분석에도 밀접한 관련"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-bobby",
   "metadata": {},
   "source": [
    "- NLP Trends\n",
    "    - 2,3년전까지는 CNN 과 GAN을 이용한 image 분석이 AI 분야를 선도함\n",
    "    - 문장을 word vector들의 sequence로 파악하면 RNN 모델이 NLP 의 핵심모델\n",
    "    - LSTM, GRU(RNN기반 모델) --- ('Attention is all you need') ---> Transformer(self-attention기반 모델)\n",
    "    - 이전 룰기반 번역은 예외상황이나 너무나 많은 언어의 사용패턴을 적용하기가 불가능했지만 DL기반으로 해결\n",
    "    - 이전에는 각기 다른 tast에 따른 DL model이 존재했었지만, 현재는 transformer를 대규모로 쌓아놓은 뒤 자가 지도학습(label이 필요치 않음)을 거친뒤 전이학습 시켜 개별 task에 적용 추세"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-printing",
   "metadata": {},
   "source": [
    "#### Lecture 2. Bag-of-Words Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-cheat",
   "metadata": {},
   "source": [
    "- Step 1: 개별 단어들로 이루어진 집합 구성\n",
    "- Step 2: 각각의 단어를 categorical variable로 취급하여 one-hot vector로 취급\n",
    "    - 각각의 단어의 Euclid 거리는 $\\sqrt{2}$ , 내적은 0\n",
    "    - 단어의 의미와 상관없이 모두 독립적인 단어로 취급\n",
    "- Stpe 3: one-hot vector 들의 합으로 문장을 표현함(bag-of-words)\n",
    "- Bayes'Rule 을 적용하여 문서 분류\n",
    "    - $ C_{MAP} = \\underset{c \\in C}{argmax}P(c|d)$ (MAP : maximum a posteriori)\n",
    " $ = \\underset{c \\in C}{argmax}\\frac{P(d|c)P(c)}{P(d)}$ (Bayes's Rule)\n",
    " $ = \\underset{c \\in C}{argmax}P(d|c)P(c)$ (c와 상관없는 항 제거)\n",
    "    - $P(d|c)P(c) = P(w_1,w_2,...,w_n|c)P(c)$ (d 를 구성하는 단어가 w1~wn 일 때) <br>$ = P(c)\\prod_{w_i\\in W} $ (각 단어가 독립일 때)\n",
    "    - 등장하지 않았던 단어들은 확률이 0이되므로 이를 보완하기 위한 regularization 기법이 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-governor",
   "metadata": {},
   "source": [
    "#### Lecture 3. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-lodge",
   "metadata": {},
   "source": [
    "Word2Vec\n",
    "- 같은 문장에서 나타난 인접한 단어들간 의미가 비슷할 것이라는 가정\n",
    "- 단어를 벡터로 표현하며 비슷한 단어끼리 작은 거리에 위치하도록 함\n",
    "- 문장을 의미단위로 토큰화를 수행하고, unique 한 단어 사전 구현\n",
    "- 한 단어를 중심으로 앞뒤로 나타난 word 각각과 입출력 쌍을 구성\n",
    "- embedding layer : one-hot vector 와 첫번 째 선형벡터(W1)의 곱\n",
    "    - 행렬곱이 아닌 vector의 index에 해당하는 행렬의 column 추출\n",
    "- W2와 행렬곱을 실시하고 softmax를 취한 결과가 ground truth 부분만 $\\infty$ 나머지는 $-\\infty$ 이어야 올바른 결과\n",
    "- 예시 사이트 : https://ronxin.github.io/wevi/\n",
    "    \n",
    "- words 간의 관계가 둘을 두 단어를 잇는 벡터로 나타내어짐\n",
    "    - ex) vec[queen] - vec[king] = vec[woman] - vec[man]\n",
    "    - 예시 : https://word2vec.kr/search/\n",
    "- intrusion detection : 단어들 중 가장 관련없는 단어하나 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-thriller",
   "metadata": {},
   "source": [
    "Word2Vec의 두가지 모델\n",
    "- CBOW(Continuous Bag-of-Words)\n",
    "    - 주변 단어로 중심 단어를 예측\n",
    "    - 주변 단어들의 one-hot encoding 벡터를 각각 embedding layer에 projection하여 각각의 embedding 벡터 얻음\n",
    "    - embedding들을 element-wise한 덧셈으로 합친 뒤, 다시 linear transformation 하여 중심 단어의 one-hot encoding 벡터와 같은 사이즈의 벡터로 만듬\n",
    "    - 중심 단어의 one-hot encoding 벡터와의 loss 계산\n",
    "    - \n",
    " \n",
    "- Skip-gram\n",
    "    - 중심 단어를 가지고 주변 단어들을 예측\n",
    "    - 중심 단어의 one-hot encoding 벡터를 embedding layer에 projection하여 해당 단어의 embedding 벡터를 얻음\n",
    "    - 이 벡터를 다시 linear transformation하여 예측하고자 하는 각각의 주변 단어들과의 one-hot encoding 벡터와 같은 사이즈의 벡터로 만듬\n",
    "    - 주변 단어들의 one-hot encoding 벡터와의 loss를 각각 계산\n",
    "    예) A cute puppy is walking in the park. & window size: 2\n",
    "        - 중심 단어: \"puppy\"        ===> CBOW 의 Output, Skip-gram 의 Input\n",
    "        - 주변 단어: \"A\", \"cute\", \"is\", \"walking\"     ===> CBOW 의 Input, Skip-gram 의 Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-newport",
   "metadata": {},
   "source": [
    "GloVe\n",
    "- 단어쌍이 동시에 등장하는 횟수를 미리 계산하고 이에 대한 로그값을 직접 곱해주어 학습을 진행함\n",
    "- cf) 빈번하게 나오는 입력-출력 단어쌍이 자주 학습되어 벡터 내적값이 커지는 것이 Word2Vec\n",
    "- 계산량이 줄어들어 빠른 학습 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-curve",
   "metadata": {},
   "source": [
    "#### peer Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-checkout",
   "metadata": {},
   "source": [
    "자연어처리 및 감성처리에 대한 고찰\n",
    "- Word2Vec, GloVe 어떤 상황에서 더 좋을까?\n",
    "- 반어법이나 특수한 케이스에 대한 자연어처리에 대하여\n",
    "1. Word2Vec, GloVe를 사용한 NLP에서 {신조어, 특수문자, 문법파괴} 문장 등에 대한 처리는 어떻게 할까?\n",
    "(문재훈님의 토이프로젝트 경험 인용)\n",
    "- 전처리하여 generalize된 문장을 사용한 방법과 특수문자(예_^^7) 등을 그대로 사용하는 방법에 대하여 각각 NLP process를 진행했을 때, 원래의 문자 그대로 사용하는 것이 더 성능이 높게 나왔다.\n",
    "- 만약 이러한 문장들에 대해 처리해야 한다면 stopword설정에 조금 더 주의를 두고 진행해야 할 것 같다.\n",
    "- Word2Vec, GloVe를 상황에 따라 달리 쓴다기보다는, word embedding에 따른 결과를 보고 더 좋은 쪽을 택하는 방향으로 선택하였다. \n",
    "2. Sentiment analysis에서 이를 활용할 때 반어법 처리도 가능할까?\n",
    "- 실제로는 안좋지만 `좋은 척` 하는 내용들도 많다. 하지만 앞뒤 문맥 없이 짧은 글에서 이를 판단하기에는 아리송한 부분이 있고 이러한 데이터들을 학습했을 때 좋은 성능을 띄지 못하더라.\n",
    "- 만약 영화평점 및 리뷰를 토대로 한 감성분석을 주제로 한다면, 이러한 데이터들은 오히려 이상치데이터로 분류하고 미리 제거해주는 게 좋지 않을까.\n",
    "\n",
    "kaggle 및 dacon에서 마음에 드는 주제를 골라서 진행중. 아직 하나의 주제에 대해 같이 토론하거나 대회를 나가지는 않는다. 다들 아직 미숙하다고 생각하여 조금 더 고민해보고 토론할 계획."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-bermuda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
