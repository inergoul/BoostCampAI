{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "increased-suffering",
   "metadata": {},
   "source": [
    "#### Lecture 1-1 : Optimazation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-subdivision",
   "metadata": {},
   "source": [
    "Generalization : 일반화, 학습에 사용하지 않은 모델에도 좋은 성능을 보이는가\n",
    "- 반복 횟수가 커질수록 training data error는 줄어들지만, test data error는 보장 못함\n",
    "- underfitting vs overfitting\n",
    "- cross-validation : 독립적인 test set 에 대해서도 잘작동하는 지 평가하기 위한 기술\n",
    "    - 이 기법으로 최적의 hyperparameter를 찾고 validation data 도 training에 사용\n",
    "\n",
    "Bias & Variance\n",
    "- low variance : 분산이 낮은 비슷한 출력값이 도출됨\n",
    "- low bias : 출력값의 평균이 target에 근접함\n",
    "- tradeoff : Cost = bias^2 + variance + noise\n",
    "\n",
    "- Bootstrapping : random sampling\n",
    "- Bagging(Bootstrapping aggregating)\n",
    "    - 여러 모델이 bootstrapping으로 학습됨(앙상블)\n",
    "    - 모든 데이터를 쓴 하나의 모델보다 여러 모델의 평균치를 이용하는 것이 좋은 성능을 보일 가능성 (parallel)\n",
    "- Boosting : 하나의 모델을 돌려보고 학습이 잘 일어나지 않는 데이터(weak learner) 위주로 모델 재구성 (sequential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-france",
   "metadata": {},
   "source": [
    "Gradient Descent : $ W_{t+1} \\leftarrow W_t - \\eta g_{t} $\n",
    "- 미분 가능한 함수에 대해 반복적으로 1차 미분값만큼 감소시켜 local minimum 찾기\n",
    "- learning rate를 적절하게 설정하기가 어려움\n",
    "\n",
    "Gradient Descent 방법론\n",
    "- stochastic GD : single sample에 대한 gradient update\n",
    "- mini-batch GD : subset of data에 대한 gradient update\n",
    "- batch gradient descent : whole data에 대한 gradient update\n",
    "- batch size 가 커지면 sharp minimizer에 도달\n",
    "- batch size 가 작아지면 flat minimizer에 도달(권장)\n",
    "\n",
    "- Momentum : $ W_{t+1} \\leftarrow W_t - \\eta a_{t+1} $ .... $ a_{t+1} \\leftarrow \\beta a_t + g_t $ ($\\beta$:momentum)\n",
    "    - 전단계의 gradient 값을 보관하여 연산에 활용\n",
    "\n",
    "- Nesterov Accelerated Gradient :  $ W_{t+1} \\leftarrow W_t - \\eta a_{t+1} $ .... $ a_{t+1} \\leftarrow \\beta a_t + \\nabla\\mathcal{L}(W_t - \\eta\\beta a_{t+1}) $\n",
    "    - 가중치가 변경된 지점의 gradient(lookahead gradient) 사용하여  converging이 빠름\n",
    "\n",
    "- Adagrad : $W_{t+1} \\leftarrow W_t - \\frac{\\eta}{\\sqrt{G_t+\\epsilon g_t}}$ ( $G_t$ : sum of gradient squares)\n",
    "    - 이전까지 많이 변한 parameter는 변화량이 감소하게 됨\n",
    "    - 뒤로 갈수록 학습이 잘 안일어나는 문제점\n",
    "\n",
    "- Adadelta : exponential moving average를 통해 Adagrad의 문제 해소, no learning rate\n",
    "\n",
    "- RMSprop : $W_{t+1} \\leftarrow W_t - \\frac{\\eta}{\\sqrt{G_t+\\epsilon g_t}}$ .... $ G_t = \\gamma G_{t-1} + (1-\\gamma)g_t^{2} $\n",
    "    - 논문자료가 아니고 해보니까 잘되서 사용하게 됨, EMA, learning rate 동시 사용\n",
    "\n",
    "- Adam(Adaptive Moment Estimation)\n",
    "    - 이전 gradients(EMA)와 squared gradients(Momentum) 모두 사용\n",
    "    - 가장 보편적인 성능 보장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-friday",
   "metadata": {},
   "source": [
    "Regularization - 학습데이터 뿐 아니라 test set 에도 잘 적응하도록 제약을 걸어줌\n",
    "- 과적합 되기 이전에 early stopping 되도록 validation data 추가 필요\n",
    "\n",
    "Parameter Norm Penalty : $ total cost = loss(\\mathcal{D};W) + \\frac{\\alpha}{2}\\lVert W \\rVert_2^2 $\n",
    "- function space 를 smooth 하게 만듬(smooth 할수록 general 한 모델이 될것)\n",
    "\n",
    "Data Augmentation\n",
    "- 데이터는 다다익선\n",
    "- preserved label data augmentation : 데이터의 정답이 바뀌지 않는 범위내에서 데이터 변형\n",
    "    - 강아지는 회전, 확대, 축소 시켜도 강아지\n",
    "    - Noise robustness : input이나 weight에 random noise 추가\n",
    "- mixup : image와 label을 동시에 섞음\n",
    "- cutout : 하나의 이미지에서 일부 제거\n",
    "- cutmix : 하나의 이미지에서 일부 제거하고 그 부분을 섞인 이미지로 채움\n",
    "- 이유를 설명하긴 어려우나 이러한 방법론을 적용하면 성능 향상\n",
    "\n",
    "Dropout\n",
    "- neuron의 일정비율을 0으로 변경\n",
    "\n",
    "Batch normalization : layer가 깊을수록 성능향상에 도움"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-referral",
   "metadata": {},
   "source": [
    "#### Lecture 1-2 : Optimization 실습 및 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-beatles",
   "metadata": {},
   "source": [
    "Optimization 함수 비교 : SGD, Momentum, ADAM\n",
    "- https://colab.research.google.com/drive/1gnL1zI6gz2uj2Exdh8Mqs6t8WqNi4ktA?usp=sharing\n",
    "- ADAM이 압도적인 성능을 보임\n",
    "- SGD는 sharp minimum을 벗어나기 어렵고, 수렴 속도가 너무 느림\n",
    "- adam으로 수렴여부를 판단하고, 정확성을 높이기 위해 다른 optimizer를 사용가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-press",
   "metadata": {},
   "source": [
    "#### Lecture 2 : CNN 원리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-indiana",
   "metadata": {},
   "source": [
    "다층신경망(MLP) : 각 뉴런들이 선형모델과 활성함수로 모두 연결된 구조(fully connected)\n",
    "- 가중치 행렬의 요소 숫자가 출발노드 수 x 도착노드 수 이므로 연산량 방대\n",
    "\n",
    "Convolution 연산 : 커널을 입력벡터 상에서 움직여가면서 선형모델과 합성함수가 적용됨\n",
    "- 커널 사이즈는 고정이므로 입력노드의 수가 커졌을 떄 parameter size를 줄일 수 있음\n",
    "- 수학적 의미 : 신호를 커널을 이용하여 국소적으로 증폭/감쇠 시킴으로써 정보 추출, 필터링\n",
    "- 실제 엄밀한 계산은 convolution보다는 cross correlation 연산임\n",
    "\n",
    "2-D Conv : 이미지 데이터 처리시 사용, kernel을 입력벡터 상에서 움직여가며 연산 적용\n",
    "- $ [f*g](i,j) = \\sum_{p,q}f(p,q)g(i+p,j+q) $\n",
    "- 입력 크기(I<sub>H</sub>, I<sub>W</sub>), 커널 크기(K<sub>H</sub>,K<sub>W</sub>), 출력 크기(O<sub>H</sub>, O<sub>W</sub>) 일 때\n",
    "    - $ O_H = I_H - K_H + 1 $\n",
    "    - $ O_W = I_W - K_W + 1 $\n",
    "- 채널이 여러개인 2차원 입력의 경우 : 2차원 conv를 채널 개수 만큼 적용, 3차원 부터는 텐서"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-awareness",
   "metadata": {},
   "source": [
    "Conv 연산의 역전파\n",
    "- 커널이 모든 입력데이터에 공통으로 적용되므로 역전파 계산시에도 convolution 연산\n",
    "- 입력 x, 커널 w, 출력 o .... $ o_i = \\sum_j w_jx_{i+j-1} $\n",
    "- $ \\partial{\\mathcal{L}}/ \\partial{w_i} = \\sum_j \\delta_j x_{i+j-1} $  convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-dependence",
   "metadata": {},
   "source": [
    "#### peer session\n",
    "\n",
    "- Bias and Variance\n",
    "    - 어떻게 cost가 어떤식으로 bias와 variance에 대해서 수식적으로 tradeoff 되는지 \n",
    "- Batch-size Matters\n",
    "   - Flat Minimum과 Sharp Minimum의 f(x) 함수가 무엇을 의미하는지\n",
    "   - small batch 와 large batch 의 상대적인 차이 (전체 데이터 셋 대비 배치의 크기)\n",
    "- 딥러닝의 방향성\n",
    "   - 딥러닝은 먼저 예측을 잘 하는 모델을 만들고, 그 뒤에 수식적인 증명이 뒤따른다.(ex GAN)\n",
    "   - 다른 분야는 보통 수식적으로 증명하고 모델을 만듬, explanable AI >> 수학, 과학의 영역\n",
    "   - 정확도를 높이는 학습 방법(Noise Robustness, Batch Normalization 등)은 성능을 올린다고 실험적으로 '알려져있다.'\n",
    "- 딥러닝이 머신러닝과 구분되는 특징\n",
    "    - 색다른 이유의 오류가 발생가능하다(의료, 경영전략 마케팅에 사용이 어려움)\n",
    "    - 효율이 압도적이므로 편의성이 있음\n",
    "    - 딥러닝이 머신러닝에 포함되는 개념으로 시작했지만 머신러닝이 traditional 딥러닝이 되어가는 추세\n",
    "- 신경망의 back propagation을 수식으로 정리해서 발표 - '조원', '백진우' 캠퍼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
