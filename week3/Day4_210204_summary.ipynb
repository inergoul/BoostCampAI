{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "canadian-leeds",
   "metadata": {},
   "source": [
    "#### Lecture 1. RNN (Recurrent Neural Network) math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-tender",
   "metadata": {},
   "source": [
    "시퀀스 데이터 : 소리, 문자열, 주가 등 연속적으로 이어진 데이터\n",
    "- 시계열(time-series) 데이터는 시간순서에 따라 나열된 데이터로 시퀀스 데이터의 일종\n",
    "- 시퀀스 데이터는 독립동등분포(i.i.d.) 가정을 위배\n",
    "- 순서를 바꾸거나 과거 정보에 손실이 발생하면 데이터의 확률분포도 변화함\n",
    "- 조건부 확률 이용 $P(X_1, ..., X_t) = P(X_t|X_1,...,X_{t-1})P(X_1,...,X_{t-1}) $\n",
    "$= ... = \\prod_{s=1}^tP(X_s|X_{s-1},...X_1)$\n",
    "- 길이가 가변적인 데이터를 다룰 수 있는 모델이 필요\n",
    "- 시퀀스 데이터를 분석할 때는 모든 과거 정보가 필요한 것은 아님\n",
    "    - 고정된 길이 $\\tau$만큼의 시퀀스만 사용하는 경우 AR($\\tau$) (Autoregressive) 자귀회귀모델\n",
    "    - 직전 정보를 제외한 나머지 정보들을 잠재변수로 인코딩해서 활용하는 잠재 AR모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-bunch",
   "metadata": {},
   "source": [
    "Recurrent Neural Network\n",
    "- 가장 기본적인 RNN 모형은 MLP와 유사하나 과거의 정보를 다룰수 없음\n",
    "- $O_t = H_tW^{(2)}+b^{(2)} ***** H_t = \\sigma(X_tW^{(1)}+b^{(1)})$\n",
    "- 여기서 $H_t = \\sigma(X_tW^{(1)}+H_{t-1}W_{H}^{(1)}+b^{(1)})$\n",
    "- 가중치 행렬 W 값은 t에 대해 불변임, H,O,X 만 t에 따라 변함\n",
    "- BPTT : backpropagation through time RNN의 역전파 방법\n",
    "    - 시퀀스 길이가 길어지면 BPTT를 통한 계산이 불안정해짐(0에 근접하면 gradient 소실)\n",
    "    - truncated BPTT : 미래의 몇개의 정보에 대해서만 블럭화하여 역전파에 사용\n",
    "    - LSTM, GRU : 좀 더 발전한 형태의 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-knife",
   "metadata": {},
   "source": [
    "#### Lecture 2. RNN 이론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-mambo",
   "metadata": {},
   "source": [
    "sequential data : 말, 동작 등 연속적인 사건\n",
    "- 우리가 얻고 싶은건 하나의 label, input의 차원을 정의하기 어려움\n",
    "- Markov model(first order AR model) : 현재는 바로 전 과거에만 dependent 함\n",
    "    - $p(x_1, ... , x_T) = \\prod_{t=1}^Tp(x_t|x_{t-1})$,  많은 정보 무시\n",
    "- Latent AR model  $\\hat{x} = p(x_t|h_t) *** h_t = g(h_{t-1},x_{t-1})$  ($h_t$ : 과거 정보 요약)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-disco",
   "metadata": {},
   "source": [
    "RNN\n",
    "- 사실상 시간순으로 풀게 되면 input width가 큰 fully-connected layer 로 묘사가능\n",
    "- short-term dependencies : 한참 전의 정보는 잘 전달되지 않음\n",
    "- LSTM(Long-short term memory)\n",
    "    - 입력 : input, previous cell state, previous hidden state // 출력 : hidden state\n",
    "    - 3 gate : Forget gate, Input gate, Output gate\n",
    "        - Forget gate : 어떤 정보를 버릴지 결정\n",
    "        - Input gate : 어떤 정보를 cell state 에 저장할지 결정\n",
    "        - Update cell : $ C_t = f_t * C_{t-1} + i_t * \\tilde{C_t} $\n",
    "        - Output gate : 위에서 update된 cell state를 이용하여 출력 만들기\n",
    "\n",
    "- GRU(Gated Recurrent Unit)\n",
    "    - No cell state, just hidden state // 2gate(reset gate and update gate)\n",
    "    - 간단한 구조로, 적은 parameter를 가짐\n",
    "- RNN이 transformer 로 대체되는 추세"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-feeling",
   "metadata": {},
   "source": [
    "LSTM 실습\n",
    "- https://colab.research.google.com/drive/1dho6gl1i3tcyYPnXB3Cl8WaJoE-ZtNJX?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-aviation",
   "metadata": {},
   "source": [
    "#### Lecture 3. Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-explorer",
   "metadata": {},
   "source": [
    "Transformer\n",
    "- 기존 sequential model의 문제점 : trimmed, omitted, permutated sequence를 다루기 힘듬\n",
    "- \"Attention is all you need\"(2017) 논문에서 transformer 개념발표\n",
    "- Neural machine translation, NMT (신경망 기계번역에 처음 쓰임)\n",
    "- 최근 대세인 GPT-3 역시 transformer의 attention 개념 사용\n",
    "- sequential data => sequential data 기존모델처럼 재귀적인 반복이 일어나는 것이 아니라 하나의 모델로 처리(출력은 AR 이 일어나지만 입력은 한번에 처리)\n",
    "- 출력값이 입력값 중 하나에만 대응되는 것이 아니라 그 외의 다른 입력값에도 종속성이 있음\n",
    "\n",
    "Attention 개념\n",
    "- 하나의 입력벡터에 대해서 다른 입력벡터와 유사도를 내적을 통해 판단하는 score변수 생성\n",
    "- score를 벡터 차원으로 나눠어준 뒤, softmax로 분류함\n",
    "- 기존 MLP나 CNN은 input 이 정해지면 출력이 고정됨\n",
    "- transformer는 하나의 input network 가 고정되어 있어도 주변 input에 의해 출력이 달라짐\n",
    "- 더 flexible 한 모델이지만 input 크기가 커지면 계산량이 제곱에 비례하여 커지는 한계\n",
    "- MHA(multi headed attention) - 하나의 encoding에 대해 여러 개의 attention 작업을 함\n",
    "\n",
    "Transformder\n",
    "- input의 각각의 단어에 대해 embedding, 8개의 head로 나누고 각각을 selt attention을 한뒤 encoding된 vector를 원래의 dimension으로 축소\n",
    "- order independent 이므로 positional encoding이 필요(위치 정보를 담은 숫자 더하기)\n",
    "- transformer는 decoder에 topmost encoder의 key, value 전달\n",
    "- 뒷 단어에 독립적이고 앞단어들과 현재단어 key, value에만 영향을 받는 출력\n",
    "\n",
    "Vision Transformer\n",
    "- image를 특정 subpicture로 나누어 linear 하게 배열하여 하나의 입력으로 처리\n",
    "- DALL-E : 문장에 대한 이미지를 생성함, GPT-3 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-nepal",
   "metadata": {},
   "source": [
    "실습 및 과제\n",
    "- https://colab.research.google.com/drive/1vg1dqa2JTHD9OFxwZfv-CwFfN3AF-Hvb?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-baghdad",
   "metadata": {},
   "source": [
    "#### peer session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-nudist",
   "metadata": {},
   "source": [
    "- LSTM에서 왜 ReLU 함수를 쓰지 않는가?\n",
    "    - ReLU를 쓰게되면 exploding gradient 문제가 생기게 된다.\n",
    "- LSTM에서는 activation function으로 tanh와 sigmoid가 같이 쓰이는데 그 이유가 무엇인지?\n",
    "    - Gradient를 더 크게 보존하기 위함\n",
    "        - forget(sigmoid): 얼마나 잊을지\n",
    "        - input(sigmoid): 얼마나 기억할지\n",
    "        - output(tanh): cell state를 기반으로 activation\n",
    "- LSTM연산에서 대괄호[ ] 연산\n",
    "    - concatenate라고 생각하면 h의 column 갯수는 time step에 따라 계속 증가\n",
    "    - W_i 가 사실 W_h와 W_x로 나뉨 (W_hi * h_t+1)  + (W_xi * x_t) (수식의 편의성)\n",
    "- batch_first = true\n",
    "    - train 시킬 때 batch size가 앞에 위치하면 코드를 작성할 때 편리\n",
    "- box, semantic segmentation 비교\n",
    "    - 결과론 적으로는 후자가 더 좋지만, 연산량과 목적에 따라 선택\n",
    "- 다음주 쉬는 동안(연휴) 할 일 내일까지 정리\n",
    "    - 데이콘, 캐글 데이터 등 분석 혹은 하고 싶은 주제(강화 학습)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-crown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
