{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "radical-selection",
   "metadata": {},
   "source": [
    "#### Lecture 1. Bayesian Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-digit",
   "metadata": {},
   "source": [
    "조건부 확률\n",
    "- $ P(A\\cap B) = P(B)P(A|B) $\n",
    "- 조건부확률 $P(A|B)$ 는 사건 B가 일어난 상황에서 사건 A가 발생할 확률\n",
    "\n",
    "베이즈 정리\n",
    "- 조건부확률을 이용하여 정보를 갱신하는 방법\n",
    "- $P(B|A) = \\frac{P(A\\cap B)}{P(A)} = P(B)\\frac{P(A|B)}{P(A)} $\n",
    "- A라는 새로운 정보가 주어졌을 때 $P(B)$로부터 $P(B|A)$ 계산하는 방법 제공\n",
    "- $P(\\theta|\\mathfrak{D}) = P(\\theta)\\frac{P(\\mathfrak{D}|\\theta)}{P(\\mathfrak{D})}$    ($ posterior = prior\\times\\frac{likelihood}{evidence}$)\n",
    "- 사후확률(데이터가 주어진 상태 확률) = 사전확률 x 가능도/데이터 자체의 분포"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-listening",
   "metadata": {},
   "source": [
    "베이즈 정리 예제\n",
    "- 암 발병률 10%, 실제로 걸렸을 때 검진될 확률 99%, 실제로 걸리지 않았을 때 오검진 확률 1%, 어떤 사람이 암에 걸렸다고 검진결과가 나왔다면 정말로 암에 걸렸을 확률은?\n",
    "- $\\theta$ 를 암발병 사건으로 정의(관찰불가), $\\mathfrak{D}$ 를 테스트 결과라고 정의(관찰가능)\n",
    "- $P(\\theta) = 0.1, P(\\mathfrak{D}|\\theta) = 0.99, P(\\mathfrak{D}|\\neg\\theta) = 0.01$\n",
    "- $P(\\mathfrak{D}) = \\displaystyle\\sum_{\\theta}P(\\mathfrak{D}|\\theta)P(\\theta) = 0.99\\times0.1+0.01\\times0.9 = 0.108$\n",
    "= $P(\\theta|\\mathfrak{D}) = 0.1\\times\\frac{0.99}{0.108} \\approx0.916$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-freeware",
   "metadata": {},
   "source": [
    "Confusion Matrix : 조건부확률의 시각화\n",
    "- True Positive : $P(\\mathfrak{D}|\\theta)P(\\theta)$\n",
    "- False Positive(1종 오류) : $P(\\mathfrak{D}|\\neg\\theta)P(\\neg\\theta)$\n",
    "- False Negative(2종 오류) : 실제로 양성인데 검사가 음성 (더 심각한 문제인 경향)\n",
    "- True Negative : 실제로 음성인데 검사가 음성\n",
    "\n",
    "주요 지표\n",
    "- $P(\\theta), P(\\neg\\theta)$ : 사전확률\n",
    "- $P(\\mathfrak{D}|\\theta)$ : 민감도(recall)\n",
    "- $P(\\mathfrak{D}|\\neg\\theta)$ : 오탐률(False Alarm)\n",
    "- $P(\\neg\\mathfrak{D}|\\neg\\theta)$ : 특이도(specificity)\n",
    "\n",
    "- 정밀도(Precision) : $P(\\theta|\\mathfrak{D}) = TP / (TP + FP) $\n",
    "\n",
    "베이즈 정리를 통해 새로운 데이터 추가 시 앞서 계산한 사후확률을 사전확률로 사용하여 사후확률 업데이트 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-fashion",
   "metadata": {},
   "source": [
    "- 조건부 확률이 유용한 통계적 해석을 제공하지만, 인과간계를 추론하는 것은 불가능\n",
    "- 데이터 분포에 변화가 있다면 예측 모형을 만들기 위해선 인과관계가 필요\n",
    "- 중첩요인의 효과를 제거하여야 인과관계 파악 가능\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-above",
   "metadata": {},
   "source": [
    "#### Lecture 2. Deep Learning 기본 용어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-motel",
   "metadata": {},
   "source": [
    "\n",
    "- 딥러닝 연구자 : Implementation skill, Math Skill(linear algebra), knowing paper\n",
    "- $ AI \\supset ML \\supset DL $\n",
    "- 딥러닝 요소 : data, model, loss function, algorithm\n",
    "- Historical Review\n",
    "    - AlexNet(2012), DQN(2013), 2014(encoder/decoder, Adam), 2015(GAN), 2017(Transformer), 2018(Bert), 2019(Big language models), 2020(self-supervised learning)\n",
    "    - hyperparameter 설정에는 설명이 불가한 부분이 있다\n",
    "    - adam은 보편적인 성능을 보장하는 방법론\n",
    "    - Deep의 의미는 network의 layer의 깊이, ResNet이 layer가 많아져도 학습이 가능토록 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-telescope",
   "metadata": {},
   "source": [
    "#### Lecture 3. pyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-resistance",
   "metadata": {},
   "source": [
    "ML framework 의 두축 - Tensorflow(google), pyTorch(facebook)\n",
    "pyTorch의 특징\n",
    "- numpy 구조를 가지는 Tensor 객체로 array 표현\n",
    "- autograd(자동미분)을 지원하여 DL 연산 지원\n",
    "- 다양한 형태의 function와 model 지원\n",
    "\n",
    "colab과 vscode 호환\n",
    "- https://www.youtube.com/watch?v=oAKxxLy-G5g\n",
    "\n",
    "간단한 pyTorch 함수 및 사용법\n",
    "- 기본적으로 torch 연산은 numpy 와 동일\n",
    "- 모르는 함수를 만나면 github에서 코드단으로 검색해볼 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-destruction",
   "metadata": {},
   "source": [
    "#### Lecture 4. Neural Networks and MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-architect",
   "metadata": {},
   "source": [
    "Neural Network\n",
    "- 인간의 신경계와 비슷한 부분이 있으나 모방하는 방법론이라고 확대해석할 필요는 없음\n",
    "- 비선형변환과 행렬연산(affine transform)이 반복적으로 이루어져 함수를 근사하는 모델\n",
    "\n",
    "Linear Neural Networks : graient descent로 loss function을 감소시키도록\n",
    "- 행렬 : 두 벡터 사이의 선형변환을 의미\n",
    "- nonliear transform 이 없으면 여러층을 쌓아도 행렬곱에 의해 한 층만 있는 것과 같은 결과\n",
    "    - activation function 도입 : ReLU / Sigmoid / Tanh\n",
    "- NN의 이론적 근거 : 어떠한 측정가능한 함수에 대해서도 일정이상의 정확성을 보장하는 은닉층이 존재함이 수학적으로 증명됨 (단, 존재성만 입증됨)\n",
    "\n",
    "Loss Function \n",
    "- Regression Task : MSE (이상치가 전체의 정확성을 해칠 우려)\n",
    "- Classification Task : Cross-Entropy\n",
    "    - 분류 문제의 output은 one-hot vector로 표현가능\n",
    "    - 분류의 관점에서 accuracy를 높이는데에 데이터의 크기는 중요치 않다, 인덱스만 중요\n",
    "- Probabilistic Task : MLE\n",
    "\n",
    "실습 및 과제\n",
    "- google colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-timber",
   "metadata": {},
   "source": [
    "#### Lecture 5. DataSet 다루기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-catalog",
   "metadata": {},
   "source": [
    "colab, ipynb 사용전에 python file 로 oop 개념을 써서 코드 구성(모듈화)\n",
    "- main(실행부), network(model 선언), config(실행 argument 저장)\n",
    "\n",
    "pytorch dataset 이 별도 존재, github에서 다운"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
