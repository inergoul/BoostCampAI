{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "victorian-alliance",
   "metadata": {},
   "source": [
    "#### Lecture 1. Instance / Panoptic segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-frame",
   "metadata": {},
   "source": [
    "1. Instance segmentation\n",
    "- instance segmentation = semantic segmentation + distinguishing instances\n",
    "- mask R-CNN : Faster R-CNN + Mask branch (각각의 class에 대해 모두 mask 생성)\n",
    "    - ROI align(소수점 단위 pixel 추출 지원)\n",
    "- YOLACT(You Only Look At CoefficienTs) - one-stage기반(R-CNN은 two-stage)\n",
    "    - mask의 prototype 을 추출해서 mask response map을 만듬\n",
    "    - detection response map을 만들어 weighted sum\n",
    "    - object class 전부를 고려하는 것이 아니라 일부의 선형결합으로 mask 생성\n",
    "\n",
    "2. Panoptic segmentation\n",
    "- 배경 정보 뿐만 아니라 물체들의 instance 까지 고려\n",
    "- UPSNet(2019) : panoptic head = semantic + instance head\n",
    "- VPSNet (for video)\n",
    "    - reference feater를 target feature map에 맞게 정렬시킴\n",
    "    - track module을 다른 객체에 연관시킴\n",
    "    - fused and tracked 모듈이 학습에 시너지를 일으킴\n",
    "\n",
    "3. Landmark localization (keypoint estimation)\n",
    "- 사람의 표정이나 동작등을 tracking하는데 주로 사용\n",
    "- coordinate regression / heatmap classification\n",
    "- landmark location to Gaussian heatmap $ G_{\\sigma}(x,y) = exp(-\\frac{(x-x_c)^2 + (y-y_c)^2}{2 \\sigma ^2}) $\n",
    "- Hourglass network : 영상을 줄여 receptive field를 줄이고 skip connection을 통해 정확한 위치 정보도 참조하는 모델(stack), stack을 여러개 반복하여 output heatmap 도출\n",
    "    - Unet 과 비슷한 구조지만 concat이 아닌 sum 을 통해 차원이 늘지 않음\n",
    "- DensePose : UV map 의 좌표 출력(3D mesh 출력)\n",
    "    - UV map : 3d geometry 를 펼쳐 2D로 flattened representation (고정된 형태의 지도)\n",
    "- RetinaFace = FPN + Multi-task branches \n",
    "\n",
    "4. Detecting object using keywords\n",
    "- cornernet - bounding box를 {top-left, bottom-right}로 두고 heat map과 embedding을 추출하여 같은 object는 embedding이 동일하다는 가정을 통해 학습\n",
    "- centernet - bounding box {top-left, bottom-right, center}, 둘다 속도가 매우 빠름\n",
    "\n",
    "대부분의 모델이 backbone network 위에 target task를 덧씌우는 형태이므로 기존 network를 응용하는 것이 유리함\n",
    "data input/output이 성능에 큰 영향을 미침"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-interim",
   "metadata": {},
   "source": [
    "#### Lecture 2. Conditional generative model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-procedure",
   "metadata": {},
   "source": [
    "1. Conditional generative model - user의 의도가 반영된 생성모델(ex 기계번역)\n",
    "- 기존 generative model : random sample image 생성\n",
    "- GAN model : generator / discriminator\n",
    "    - conditional GAN에서는 generator 입력에 conditional input이 추가됨\n",
    "- image-to-image translation : style transfer, super resolution, colorization\n",
    "    - super resolution : 저해상도 input => 고해상도 가짜 영상\n",
    "    - averaging answer 문제 해결\n",
    "        - ex) 흰색 + 검정색 image의 경우 보통 CNN의 L1 loss 적용시 뭉개진 회색 영상이 나오지만 GAN 사용시 회색은 기존 이미지에 없는 값이므로 discriminator에 의해 걸러짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-election",
   "metadata": {},
   "source": [
    "2. Image translation GANs\n",
    "- Pix2Pix : 번역과 같이 다른 domain에서 대응하는 이미지 추출(pairwise data)\n",
    "    - GAN loss 와 L1 loss 함께 사용\n",
    "    - $ G^* = argmin_G max_D \\mathcal{L}_{cGAN}(G,D) + \\lambda \\mathcal{L}_{L1}(G)$\n",
    "- CyclaGAN : non-pairwise dataset 끼리 학습가능하도록 만듬\n",
    "    - cycleGAN loss = GAN loss(in both direction) + cycle-consistency loss\n",
    "    - GAN loss만 사용시 input에 상관없이 generator가 같은 output 출력(mode collapse)\n",
    "    - cycle-consistency-loss를 통해 역방향 출력시 contents가 유지되도록\n",
    "- perceptual loss : 기존 GAN(adversarial) loss가 학습이 어렵다는 단점을 극복\n",
    "    - loss measure를 위해 pre-trained network 가 요구됨\n",
    "    - pre-trained classifier가 인간의 지각처럼 filter response를 가지고 있음\n",
    "    - image transform net : output a transformed image from input\n",
    "    - transforming image와 content target을 pre-trained loss network(ex VGG)에 통과시킨 후 두 feature map 사이의 loss를 측정\n",
    "    - 같은 작업을 conten target 대신 style target에 대해 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-interim",
   "metadata": {},
   "source": [
    "3. Various GAN applications\n",
    "- Deepfake : StyleGAN 의 대표적인 사례, fake image, fake speech 생성 가능\n",
    "    - 악용의 우려가 있어서 detection challenge도 열리고 있음\n",
    "- face de-indetification : password\n",
    "- video translation : pose transfer(CG 효과), video-to-video translation    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-implementation",
   "metadata": {},
   "source": [
    "#### peer session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-soldier",
   "metadata": {},
   "source": [
    "YOLO 논문 분석 및 구현 (by 김민수 캠퍼)\n",
    "Normalization vs Regularization (by 이종혁 캠퍼)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-invitation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
