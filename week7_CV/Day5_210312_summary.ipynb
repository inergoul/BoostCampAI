{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "split-heading",
   "metadata": {},
   "source": [
    "#### Lecture 1. Multi-modal Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-universal",
   "metadata": {},
   "source": [
    "1. Multi-modal Learning\n",
    "- 서로 다른 타입의 data를 학습에 사용(ex text, sound)\n",
    "- data representation 의 차이 (ex audio - 1d, image - 3d)\n",
    "- 1:N matching (ex text space - image space)\n",
    "- model 학습의 한계 (one modality에 편향되는 경향이 발생)\n",
    "- multi-modal 유형 : matching / translating / referencing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-lightning",
   "metadata": {},
   "source": [
    "2. Multi-modal tasks - visual data & text\n",
    "- text embedding : 문자를 machine learning에 사용하기 힘듬, map to dense vectors\n",
    "    - dense representation 은 일반화 능력이 있음\n",
    "    - word2vec : skip-gram model(중심 단어로 주변 단어 예측)\n",
    "- join embedding : text, image embedding space간의 distance를 줄이는 방향으로\n",
    "    - text data는 embedding 된 data를 softmax를 취함\n",
    "    - image data는 feature real value의 gaussian model을 구함\n",
    "    - image tagging, image + food recipe 연관, \n",
    "- cross modal translation : image captioning\n",
    "    - show and tell : encoder(CNN model), decoder(LSTM module)\n",
    "    - show attend and tell - soft attention, inference\n",
    "    - text-to-image by generative model\n",
    "- cross-modal reasoning(referencing) : 다른 modality를 참고하여 답을 도출\n",
    "    - visual question answering : CNN과 LSTM을 통과한 FC-layer를 point-wise multiplication > FC > softmax 을 배치한 후 end-to-end training\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-insert",
   "metadata": {},
   "source": [
    "3. Multi-modal tasks - visual data & audio\n",
    "- sound representation : 시간 축에 대한 waveform(1-d signal) -> spectrogram\n",
    "    - Short-time Fourier Transform(STFT) : 짧은 시간단위로 쪼개서 Hamming window 적용\n",
    "    - Fourier Transform - 파형을 주파수 성분으로 분해\n",
    "    - spectrogram : 시간 축에 따른 spectrum의 stacking, ex) MFCC, malspectrogram\n",
    "- join embedding : sound tagging\n",
    "    - SoundNet : train by teacher-student manner, poo5 feature 로 classifier 훈련\n",
    "- cross modal translation :\n",
    "    - Speech2Face : video 내의 얼굴과 목소리가 이미 매칭이 되어 있어 학습에 사용\n",
    "    - image-to-speech synthesis\n",
    "        - mage > subword unit(Resnet, LSTM + attention) + unit > speech(TTS)\n",
    "        - subword unit을 기점으로 양방향 학습\n",
    "- cross-modal reasoning(referencing) : \n",
    "    - sound source localization : 영상에서 소리와 매칭되는 sementic object를 표시\n",
    "        - visual net/audio net ==> attention net , localization score로부터 학습\n",
    "        - image-audio matching을 annotation으로 활용하여 unsupervised learning 가능\n",
    "    - speech separation : 각각의 speaker에 대해 spectrogram mask 를 출력\n",
    "        - training data를 두개의 clean speech video로부터 합성함\n",
    "\n",
    "4. Conclusion\n",
    "- text,audio에 국한되지 않고 다양한 sensor의 datatype을 처리하는 것도 가능\n",
    "    - ex) Autopilot : tesla self-driving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-replica",
   "metadata": {},
   "source": [
    "#### image captioning 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "great-cowboy",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torchvision' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6938b9d9e3a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# image captioning 구현\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresnet101\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# classification task가 아니므로 위치 정보 보존을 위해 linear, pool layer 보존\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torchvision' is not defined"
     ]
    }
   ],
   "source": [
    "# image captioning 구현\n",
    "resnet = torchvision.models.resnet101(pretrained=True)\n",
    "modules = list(resnet.children())[:-2]\n",
    "# classification task가 아니므로 위치 정보 보존을 위해 linear, pool layer 보존\n",
    "self.resnet = nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-japanese",
   "metadata": {},
   "source": [
    "- beam search k = 3 적용 : each decode step 에 대하여 3가지 top score에 대해 3가지 경우의 수, 총 3x3 = 9가지 중에 가장 score 높은 3가지만 남겨둠 (여지가 있는 greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-journal",
   "metadata": {},
   "source": [
    "#### Lecture 2. 3D understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-brand",
   "metadata": {},
   "source": [
    "1. 3D 관점의 세계\n",
    "- 우리가 살고 있는 세상이 3D 이므로, AI model도 3D 공간에 대한 이해가 필요\n",
    "- AR / VR, 3D medical application, 단백질 구조 분석\n",
    "- image는 2D projection ( ex - camera : 2D projection 도구 )\n",
    "- triangulation : 2D image들로 3D image 구하기\n",
    "- 3D representation : multi-view image, volumetric, point cloud, part assembly, mesh(Graph CNN 여러개의 삼각형으로 쪼개기), implicit shape(고차원 함수 F(x) = 0)\n",
    "- 3D dataset\n",
    "    - ShapeNet : 51,300 3D model with 55 category\n",
    "    - PartNet : 573,585 part instance in 26,671 3D model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-evaluation",
   "metadata": {},
   "source": [
    "2. 3D tasks\n",
    "- 3D object recognition : 2D image 처럼 volumetric CNN 이용하여 구분\n",
    "- 3D semantic segmentation\n",
    "- conditional 3D generation : detected objects, Mask R-CNN(3D branch 추가)\n",
    "- 복잡한 3D reconstruction model >> 여러 sub-problem(물리적 단위) 으로 분해"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-sucking",
   "metadata": {},
   "source": [
    "3. 3D Application example\n",
    "- photo refocusing(ex portrait mode) : depth map thresholding(focus/defocus mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-hebrew",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
