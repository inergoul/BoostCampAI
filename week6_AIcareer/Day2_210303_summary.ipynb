{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "shared-strand",
   "metadata": {},
   "source": [
    "#### Lecture 1. 캐글 경진대회 그랜드마스터의 노하우 by 김상훈"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-bulletin",
   "metadata": {},
   "source": [
    "1. 캐글 소개\n",
    "- 2010년 설립된 세계에서 가장 유명한 인공지능 대회 플랫폼\n",
    "- 비슷한 국내 대회로 카카오 아레나, 데이콘이 있음\n",
    "- 캐글의 목적 : 세계적으로 실력을 인정받기 위해(취업목적), AI개발자로 배우고 성장\n",
    "    - 랭킹 시스템과 티어시스템으로 실력을 인정받음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-quilt",
   "metadata": {},
   "source": [
    "2. 캐글 시작해보기\n",
    "- 회원가입, 참여할 대회 선택, 데이터 다운로드, 파이프라인 구축\n",
    "- 대회 제출 방식\n",
    "    - General competition(리소스 제약 없음), submission.csv 파일만 제출\n",
    "    - code competition(리소스 제약 있음), 캐글 노트북에서 코드를 실행해야함\n",
    "- 파이프라인 : 데이터 전처리 > 학습 > 제출 준비 > 리더보드 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-mailing",
   "metadata": {},
   "source": [
    "3. 캐글 노하우대방출\n",
    "- 파이프라인의 빠르고 효율적인 반복 : GPU 장비 (블로워 타입 중고 구입 추천)\n",
    "- 1,2 달 동안 하루평균 4시간 이상 투자\n",
    "- 본인만의 base code 보유할 것\n",
    "- public에 과적합 되지 않도록 탄탄한 검증 전략 구축할 것\n",
    "    - 좋은 모델(일반화 성능이 높은 모델) : training set 점수와 test set 점수가 비슷\n",
    "    - 검증 전략(validation strategy) : test set 점수와 training set 점수 갭을 줄이는 평가 방법\n",
    "    - k-fold 검증 전략 : training set을 쪼개 validation set 만들기\n",
    "    - stratified k-fold : class별로 training set을 같은 비율로 쪼갬\n",
    "    - Ensenble : 여러 모델의 예측결과를 섞어서 결과를 향상시킴\n",
    "- 종료 1~2주전까지 싱글 모델 점수로만 50등 내에 들면 좋음\n",
    "- 버전별로 개별 폴더를 만들어 코드 관리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-authorization",
   "metadata": {},
   "source": [
    "#### Lecture 2. Full Stack Machine Learning Engineer by 이준엽"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-ireland",
   "metadata": {},
   "source": [
    "1. Full stack ML Engineer?\n",
    "    - ML 기술을 이해, 연구, product를 만드는 엔지니어\n",
    "    - machine learning researcher 와 software engineer의 중간 지점\n",
    "    - full stack : client(front-end) / API-server+database(back-end)\n",
    "    - 코딩을 잘하고 창의적이고 다른 사람과 협업이 가능한 사람\n",
    "    - ML service 로 server의 data를 이용하기도 하고 edge device service로 front-end level에서 ML model을 돌려서 처리하기도 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-leadership",
   "metadata": {},
   "source": [
    "2. Full stack ML engineer의 장단점\n",
    "- 장점\n",
    "    - 재미있다 - 처음부터 끝까지 하나의 product를 만들수 있음\n",
    "    - ML model의 빠른 prototyping\n",
    "    - 연결되는 stack에 대한 이해가 각 stack의 깊은 이해에도 도움을 줌\n",
    "    - teamplay 시에 상대 영역에 대한 이해를 통해 잠재적 위험에 대한 고려 가능\n",
    "    - 성장이 다각화 됨, 회의 내용이 성장의 밑거름, 매너리즘을 떨치는 방법이 되기도 함\n",
    "- 단점\n",
    "    - 모든 stack에서 최신 트렌드를 따라잡기가 어려워 깊이가 없어질 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-springfield",
   "metadata": {},
   "source": [
    "3. ML Product, ML Team, ML Engineer\n",
    "- ML product : 요구사항 전달 > 데이터 수집 > ML 모델 개발 > 실서버 배포\n",
    "    - 요구사항 전달 : 실생활의 문제를 ML 이 풀 수 있는 형태로 전환\n",
    "    - 데이터 수집 : Raw 데이터 수집 / Annotation tool 기획 및 개발 / Annotation Guide 작성 및 운용\n",
    "    - ML model 개발 : 기존 연구 research 및 내재화, 실데이터 적용 실험 + 평가 피드백, 모델 차원 경량화\n",
    "    - 실서버 배포 : 엔지니어링 경량화, 연구용 코드 수정, 모델 버전 관리 및 배포 자동화\n",
    "- ML team : 프로젝트 매니저, 개발자, 연구자, 기획자, 데이터 관리자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-malaysia",
   "metadata": {},
   "source": [
    "4. Roadmap\n",
    "- front-end : Vue.js + Angular // back-end : django + Flask + Rails // Machine Learning : pyTorch + TensorFlow // Database : MySQL + MariaDB + redis // Ops : docker + github + aws\n",
    "- 모든 stack이 시작이 가장 어려우니 익숙한 언어 + 가장 적은 기능 + 가장 쉬운 framework로 시작할 것\n",
    "- 처음부터 너무 잘 만들려고 하지말고 최대한 빨리 완성할 것\n",
    "- 완성된 코드에 기능을 더하다 보면 자연스레 리팩토링의 중요성에 대해 알게됨\n",
    "- 빠르게 전문분야를 정할 것\n",
    "- ML engineer 라면 하나의 논문을 구현하고 demo page 만들어 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-executive",
   "metadata": {},
   "source": [
    "#### peer session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-titanium",
   "metadata": {},
   "source": [
    "0303 피어세션 (면접질문과 연습)\n",
    "1. 차원의 저주가 무엇인지 알고있는지 설명하고, 이를 어떻게 해결할 수 있는가?\n",
    "- 차원의 저주는 모델 예측시 샘플의 수에 비해 변수의 수가 상대적으로 너무 많아서 예측성능이 떨어지는 현상을 말한다\n",
    "- Feature Selection(Importance기반, Lasso기반 Selection), Dimensional Reduction (PCA, SVD 등 latent Vector 추출 방법)\n",
    "2. 알고계신 활성화함수들을 말해주시고, 차이점은 무엇이 있는지 등 소개를 해주세요\n",
    "- 시그모이드 함수 : 입력을 넣었을 때 0~1 사이의 값을 출력하는 함수, binary label일 때 사용 (multi label에서는 soft max함수가 쓰인다)\n",
    "- 하이퍼볼릭 탄젠트 : -1~1 사이의 값을 출력하는 함수\n",
    "> 위 두가지는 gradient vanishing Problem\n",
    "- relu 함수 : 0이하에서는 0의 값을 갖고, 이외의 부분에서는 x 그대로의 값을 갖는 max(x,0)\n",
    "> leaky relu 등의 0 이하의 값에서 0이 아닌 값을 갖게 하는 활성화 함수도 있다\n",
    "3. Generalized하게 performance 를 향상시키기 위한 방법에 무엇이 있을지\n",
    "- 배치정규화\n",
    "- DropOut\n",
    "- Cross Valdation (ex. K-Fold CV)\n",
    "- L1, L2 정규화\n",
    "- Data Augmentation\n",
    "- Early Stopping\n",
    "4. CNN 에서 패딩을 쓰는 이유\n",
    "- Convolution Layer를 거치면서 이미지의 크기가 점점 작아지게 된다.\n",
    "- 이는 이미지의 가장자리에 위치한 픽셀들의 정보는 점점 사라지게 되는 문제가 발생시킨다.\n",
    "- 패딩을 컨볼루션 층 이후에 하는 것과 이전에 하는 것의 차이 : Convolution 과정에서 가장자리에 있는 element는 한번밖에 연산에 포함이 안됨.\n",
    "- 즉, 패딩을 컨볼루션 이전에 해주지 않는다면 모서리에 있는 element와 중앙쪽에 위치하는 element 사이에 연산에 포함되는 횟수가 너무 많이 차이 나게 되는 현상이 일어나버리게 된다는 것이다.\n",
    "5. 분류 문제의 신경망을 설계할 때 출력층에서 사용하는 활성화 함수와 손실함수가 무엇인지 말하고 그 사용 이유와 확률론적으로 어떤 의미를 가지는 함수인지 설명해주세요.\n",
    "- 활성화 함수로는 이중 분류의 경우에는 sigmoid를 사용하고 다중 분류의 경우에는 softmax를 사용합니다. 사용이유는 함수의 출력값이 0과 1사이의 값을 가지며 출력층값의 합이 1이 되기 때문에 이산 확률 분포를 나타내기 적합한 함수라는 점입니다. 연속함수이고 모든 점에서 미분이 가능하여 역전파 계산을 하기 수월하다는 것도 장점입니다.\n",
    "- sigmoid의 수학적 의미를 정의하기 위해선 logit 과 odd에 대한 이해가 필요한데 odds는 확률의 또다른 표현으로서 어떠한 사건이 일어날 확률을 a라 할때 a / (1-a) 즉 사건이 일어날 확률이 일어나지 않을 확률보다몇 배가 더 큰지를 의미하며 확률값의 변화에 민감하게 반응하는 지표입니다. 이 odds 에 자연로그를 취해준 함수를 logit 이라고 하며 log p - log(1-p)가 됩니다. sigmoid 함수는 바로 이 함수의 역함수로서 logistic function 이라고도 불립니다. -inf ~ +inf 범위의 값을 0 ~ 1의 확률값으로 변환하는데 충분한 의미가 있는 함수이다. softmax함수는 sigmoid함수의 일반형으로 베르누이 시행이아니라 클래스가 3개이상의 경우로 분류되는 경우를 의미하는 함수로 만약 softmax 함수에서 시그마 항이 2개 뿐인경우 sigmoid와 동일한 함수식임을 알 수 있습니다. ((즉, sigmoid는 softmax의 특수한 경우로서 만약 1~10 10개의 클래스가 존재할 때 이것을 3인 경우와 3이 아닌 경우 이런식으로 나눠서 생각할때 적용할 수 있는 함수라는 개념)) \n",
    "- 손실함수로는 cross-entropy 함수를 주로 사용하는데 우선 사용이유로는 회귀 문제에 주로 사용되는 mse(mean squared error)에 비해 오차에 대한 반응성이 크기 때문에 loss가 더 빠르게 수렴하며, gradient 소실을 해결하는데 도움이 된다. CE = - sum(ti log(si)) 로 정의가 되는데( ground truth * 예측값의 로그값 의 합에 마이너스를 해준것 ) 이산 확률 분포를 가정할 때 log likelihood를 구하면 이것이 최대화되는 경우, 즉 MLE 를 수행하였을때가 cross-entropy 가 최소가 됨을 확인할 수 있다. 파이토치에서 구현은 nn.CrossEntropyLoss로 softmax와 cross-entropy가 함께 사용된다\n",
    "\n",
    "6. normalization 이랑 regularization의 차이\n",
    "- Normalization은 정규화라는 뜻 (분포를 바꾸어주는 것, 우리가 알고있는 scaling이 그 예시)\n",
    "- regularization는 규제라는 뜻 (모델의 일반화를 위한, L1 L2 regularization, DropOut, BN 등 모든 과적합을 막기 위한 수단들이 그 예시)\n",
    "7. R = RNN (d) ; R.eval()과 같은 코드가 작성되어 있을 때 eval() 메소드가 어떤 역할을 하는지\n",
    "- 모델에 eval()이라는 메소드를 통해 Train이 아닌 Evaluation 태스크라는 것을 선언해주는 부분이다. (예를 들어, DropOut이 트레인에 적용되었었다면 evaluation 시에는 DropOut된 노드 없이 predict를 수행해야 함)\n",
    "- no_grad를 쓰는 이유 : gradient값의 저장과 계산과정이 없어지기 때문에 비용과 메모리 절약 가능\n",
    "\n",
    "번외질문) train에서 BN layer가 학습한 가중치들을 Test예측시에 어떻게 사용하는가?\n",
    "- 모델 predict시 테스트 셋에 대한 BN 과정은 Test set 에서의 평균과 표준편차를 사용하지 않고, Train 시에 업데이트하여 저장되어있던 평균과 표준편차를 활용한다.\n",
    "- 즉, Train시에 각 배치별로 업데이트된 평균들과 표준편차들이 있다면 각 배치별로 나온 평균들의 평균과 표준편차들의 평균을 계산하고, 이를 활용하여 Test set에 Normalization을 적용함으로써 Test set에서 BN 과정을 수행하게 된다.\n",
    "> 참고 : https://shuuki4.wordpress.com/2016/01/13/batch-normalization-%EC%84%A4%EB%AA%85-%EB%B0%8F-%EA%B5%AC%ED%98%84/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-heritage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
