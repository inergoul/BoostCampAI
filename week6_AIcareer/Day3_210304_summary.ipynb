{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "collected-profession",
   "metadata": {},
   "source": [
    "#### Lecture 1. 자연어 언어모델과 평가 by 박성준"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-album",
   "metadata": {},
   "source": [
    "1. 언어 모델링(language modeling)\n",
    "- 주어진 문맥을 활용해 다음에 나타날 단어 예측하기\n",
    "- $ P(w_1w_2...w_n) = \\prod_i P(w_i|w_1w_2...w_{i-1})$\n",
    "- 양방향 언어 모델링 - ELMo(Embeddings from Language Models)\n",
    "- 언어모델의 힘을 빌리면 하나의 모델로 모든 언어 task를 충족 가능\n",
    "- BERT(Bidirectional Encoder Representations from Transformers)\n",
    "    - transformer의 encoder 구조를 사용하여 대량의 corpus로 사전학습한 모델\n",
    "    - semi-supervised training : pre-training // supervised training : fine-tuning\n",
    "\n",
    "2. 언어 모델의 평가\n",
    "- GLUE(General Language Understanding Evaluation) 벤치 마크 - 영어 중심\n",
    "    - Quora Questions Pairs(QQP : 문장 유사도 평가)\n",
    "    - Question NLI(QNLI : 자연어 추론)\n",
    "    - the Stanford Sentiment Treebank(SST : 감성 분석)\n",
    "    - the Corpus of Linguistic Acceptability(CoLA : 언어 수용성)\n",
    "    - Semantic Textual Similiarity Benchmark(STS-B : 문장 유사도 평가)\n",
    "    - Microsoft Research Paraphrase Corpus(MRPC : 문장 유사도 평가)\n",
    "    - Recognizing Textual Entailment(RTE : 자연어 추론)\n",
    "- 다국적 벤치마크의 등장 : KLUE 프로젝트\n",
    "    - 개체명 인식 / 품사 태깅 및 의존 구문 분석 / 문장 분류 / 자연어 추론\n",
    "    - 문장 유사도 / 관계 추출 / 질의 응답 / 목적형 대화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-topic",
   "metadata": {},
   "source": [
    "#### Lecture 2. 내가 만든 AI 모델은 합법일까, 불법일까 by 문지형"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-tactics",
   "metadata": {},
   "source": [
    "1. 저작권법, 우리가 왜 알아야 할까\n",
    "- 웹에 있는 데이터 크롤링해서 학습에 쓰면 큰일남\n",
    "- 좋은 AI 모델은 좋은 데이터로부터 나옴\n",
    "- 현업에서는 적절한 데이터가 마련되어 있지 않아 직접 데이터를 제작해야 함\n",
    "\n",
    "2. 저작권\n",
    "- 사람의 생각이나 감정을 표현한 결과물에 대하여 창작자에게 주는 권리\n",
    "- 창작성이 있다면 별도의 등록절차없이 자연히 발생\n",
    "    ex) 판례 검색 서비스 : 판례가 저작권법의 보호를 받지 못하므로 문제가 없음\n",
    "\n",
    "3. 합법적으로 데이터 사용하기\n",
    "- 저작자와 협의 : 독점적 이용허락 / 비독점적 이용허락\n",
    "- 라이센스 : 저작물에 대한 이용허락 규약, CCL (ex CC-BY-ND/SA/NC)\n",
    "    - CC : creative commons, BY : 저작자 표시(attribution), ND : 변경 금지(NoDerivatives))\n",
    "    - NC : 비영리(NonCommercial) / SA : 동일조건 변경허락(Share Alike)\n",
    "\n",
    "4. 저작권법의 회색지대가 존재 : 모델이 생성한 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-stress",
   "metadata": {},
   "source": [
    "#### peer session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-portland",
   "metadata": {},
   "source": [
    "my question) 워드 임베딩의 개념과 워드 임베딩의 대표적인 기법인 Word to Vec에 대해 설명해 주세요.\n",
    "\n",
    "- 기존 단어를 표현하는 방법인 one-hot encoding은 표현하고자 하는 단어의 인덱스 값만 1이고 나머지 인덱스는 전부 0으로 표현하는 희소 표현(sparse representation)으로 단어 집합이 커질수록 차원이 한없이 커져 공간적 낭비를 일으킨다는 단점이 있으며 단어의 의미를 표현할 수가 없습니다.\n",
    "- 사용자가 설정한 값으로 모든 단어 벡터 표현의 차원을 맞추는 방법을 밀집 표현이라고 하는데 차원이 조밀해진 밀집 벡터(dense vector)가 생성되며 단어를 밀집 벡터의 형태로 표현하는 방법을 워드 임베딩(word embedding) 이라고 합니다. 훈련 데이터로 학습함\n",
    "- wordtovec은 비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다는 가정으로 저차원에 단어의 의미를 분산하여 표현함으로써 단어 간 유사도를 계산가능\n",
    "- CBOW 와 Skip-Gram 두가지방식이 있으며 CBOW는 주변 단어를 가지고 중간의 단어를 예측하는 방법이고 skip-gram은 중간에 있는 단어로 주변 단어들을 예측하는 방법임\n",
    "- 단어를 중심으로 몇개(window의 크기)의 단어를 볼지 결정하고 윈도우를 움직여서 주변단어와 중심단어 선택을 바꿔가며 학습을 위한 데이터셋을 만들 수 있는데 이를 sliding window 라고 함\n",
    "- word2vec의 입력은 원핫벡터가 되며 CBOW 역시 인공신경망 모델이다. 다만 딥러닝 모델은 아닌데 입력층과 출력층 사이에 하나의 은닉층만이 존재하며 활성화 함수가 존재하지 않고 룩업 테이블로서의 기능을 수행한다.(투사층이라고 부르기도 한다)\n",
    "\n",
    "그 외)\n",
    "1. standard scaler와 minmax scaler의 장단점, 단점의 해결방법\n",
    "- StandardScaler 와 MinMaxScaler는 이상치에 민감하게 scaling되어 분포를 그대로 유지하지 못하게 된다.\n",
    "- RobustScaler : 평균 대신 중앙값을 씀으로써 보다 분포형태를 유지하게끔 scaling을 할 수 있게 된다.\n",
    "- 참고 : https://mkjjo.github.io/python/2019/01/10/scaler.html\n",
    "\n",
    "2. Multi head attention에서 matmul을 사용하는데 고차원에선 이러한 곱연산이 어떻게 이루어지는가? (예를들어 3, 4차원 정도의 고차원일 경우 matmul연산이 어떻게 이루어지는가?)\n",
    "- A는 7x5x4x3 차원, B는 7x5x3x4 차원일 때 결과는 7x5x4x4의 shape이 나온다.\n",
    "- 이는 뒤의 두 dimension이 나타내는 행렬의 차원의 곱이 성립하고, 이외에 앞의 shape (위의 경우는 7x5, 7x5이므로 같음)이 모두 같다면 곱 연산이 가능하다.\n",
    "- 참고 : https://ebbnflow.tistory.com/159\n",
    "\n",
    "3. CNN에서 층을 많이 쓰면 좋다고 알려져있는데 왜 막상 많이 쌓으면 안좋은가, 이를 어떻게 해결했는가?\n",
    "- 층이 많아지면 파라미터가 계속해서 곱해지면서 작아지니까? 앞단의 정보가 희석이 되니까\n",
    "- 이를 해결하기 위해 만든 것이 ResNet 모델이다. 이것은 구조상 아웃풋에 x를 더해줌으로써 CNN층을 더욱 깊게 쌓더라도 전층의 정보를 유지할 수 있도록 함\n",
    "- 1X1 convolution layer를 쓰는 이유 : 차원은 유지하면서 파라미터 수를 줄일 수가 있다.\n",
    "\n",
    "4. LSTM과 GRU의 차이는?\n",
    "- Cell state가 없다, GRU는 게이트가 2개이고, LSTM은 3개이다. \n",
    "\n",
    "5. Bias-Variance Trade off가 무엇인지 과적합과 연관지어 설명해보기\n",
    "- Bias는 train시 y와 predict 값의 차이를 의미한다.\n",
    "- 이 Bias를 줄이는 것으로 너무 과대적합을 하게 되면 Variance가 높게 되는 경우가 생기며, 다시 Variance를 낮추기 위해 노력하면 Bias가 높아지는 현상이 생기는데 이것을 Bias-Variance Trade off라고 한다.\n",
    "- 결국 Bias와 Variance 두 가지가 합리적으로 함께 낮은 지점을 찾도록 노력해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-romance",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "clinical-possible",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "driving-wrapping",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "respected-graham",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "military-ratio",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
