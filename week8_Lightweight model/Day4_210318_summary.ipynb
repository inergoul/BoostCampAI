{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "approximate-fashion",
   "metadata": {},
   "source": [
    "#### Lecture 1. Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-disease",
   "metadata": {},
   "source": [
    "- exploitation of precisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-helmet",
   "metadata": {},
   "source": [
    "#### mini code : floating-point operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "challenging-breathing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30000000000000004 False\n",
      "0.4 False\n",
      "<class 'int'>\n",
      "0.9999999999999998 False\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "print(0.1 + 0.2, 0.1 + 0.2 == 0.3)\n",
    "print(0.1 + 0.3, 0.1 + 0.3 == 0.3)\n",
    "\n",
    "sum = 0\n",
    "print(type(sum))\n",
    "for _ in range(7):\n",
    "    sum += 1.0 / 7.0\n",
    "print(sum, sum == 1.0)\n",
    "print(type(sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-aviation",
   "metadata": {},
   "source": [
    "1. Fixed point, floating point\n",
    "- fixed point : sign + integer part + fractional part\n",
    "- floating point : sign + exponent(지수부) + mantissa(가수부)\n",
    "    - 더 넓은 범위의 숫자 처리 가능, FPU 필수, 숫자 정보량은 동일(동일-bit라면)\n",
    "- continous value, discrete value\n",
    "- precision(정밀도, variance 개념) / accuracy(정확도, bias 개념)\n",
    "\n",
    "2. Quantization 이란\n",
    "- model size 감소, inference speed 를 높이기 위한 기술, lossy conversion\n",
    "- affine quantization : 직선 상의 두 점의 거리 비를 보존하는 affine 변환\n",
    "- active function을 quantize 시에 역전파 미분이 안되는 문제\n",
    "    - $\\frac{dy}{dx} = 1$로 가정, smoothing 된 계단 함수 사용\n",
    "\n",
    "3. Quantization 예시\n",
    "- how much : 16bit, 8bit, 4bit,.. , or mixed-precision\n",
    "- how to : dynamic(activation이 inference 도중 양자화 / static\n",
    "- when : post-training(training 후 weight 값들에 대해 양자화 진행) / quantization-aware training(학습 진행 시점에 inference 시 양자화 적용 영향 simulation)\n",
    "- what to : weight, activation\n",
    "- harware aware quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-sword",
   "metadata": {},
   "source": [
    "#### Lecture 2. Knowledge distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-analyst",
   "metadata": {},
   "source": [
    "##### mini code : (hard) max, argmax, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "curious-collapse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 3 VS argmax: 1\n",
      "DATA 원소들의 softmax 전체 합 : 1.0\n",
      "DATA 원소들의 softmax 값 : 0.0900, 0.2447, 0.6652\n",
      "DATA 원소들의 softmax 전체 합 : 1.0\n",
      "DATA 원소들의 softmax 값 : 0.0900, 0.6652, 0.2447\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "DATA = [1,3,2]\n",
    "ans = np.max(DATA)\n",
    "arg_ans = np.argmax(DATA)\n",
    "print('max: {} VS argmax: {}'.format(ans, arg_ans))\n",
    "\n",
    "p1 = np.exp(1) / (np.exp(1) + np.exp(2) + np.exp(3))\n",
    "p2 = np.exp(2) / (np.exp(1) + np.exp(2) + np.exp(3))\n",
    "p3 = np.exp(3) / (np.exp(1) + np.exp(2) + np.exp(3))\n",
    "print('DATA 원소들의 softmax 전체 합 : {:.1f}'.format(p1+p2+p3))\n",
    "print('DATA 원소들의 softmax 값 : {:.4f}, {:.4f}, {:.4f}'.format(p1,p2,p3))\n",
    "\n",
    "from scipy.special import softmax\n",
    "scipy_ans = softmax(DATA)\n",
    "print('DATA 원소들의 softmax 전체 합 : {:.1f}'.format(np.sum(scipy_ans)))\n",
    "print('DATA 원소들의 softmax 값 : {:.4f}, {:.4f}, {:.4f}'.format(*scipy_ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-compromise",
   "metadata": {},
   "source": [
    "1. knowledge\n",
    "- 의사 결정이 가장 중요한 초창기에 지식이 오히려 별로 없음, 불확실성이 큼\n",
    "- logit / sigmoid / softmax 개념\n",
    "\n",
    "2. knowledge distillation\n",
    "- model에 있는 data가 무거우므로 증류하여 작은 network로 전달\n",
    "- transfer learning 은 다른 domain 사이의 전달이라면 knowledge distillation은 같은 domain에서 model의 size를 작게 만들기 위한 전달\n",
    "\n",
    "3. teacher-student loss & distillation\n",
    "- pretrained teacher model로부터 input에 대해 soft label을 얻고, student(distilled) model로부터 input에 대해 soft prediction(확률 실수값)과 hard prediction(0 / 1)을 얻음\n",
    "    - soft label 과 soft prediction 사이에 distillation loss\n",
    "    - hard label(ground truth) 과 hard prediction 사이에 student loss\n",
    "    - softened label : 확률 값 사이의 유사성도 학습에 사용하기 위함\n",
    "\n",
    "4. zero-mean assumption\n",
    "- teacher-student 를 통해 나온 distribution이 zero mean을 가질 때 각각의 logit에 따라 gradient를 정의하게 되고 model compression 이 knowledge distillation 의 특수한 경우임을 알 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-indonesia",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sorted-cherry",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "experimental-effort",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "introductory-noise",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "utility-liverpool",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-harbor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
